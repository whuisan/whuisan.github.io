# -*- coding: utf-8 -*-
"""SWA Coding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gk9bGSKS5T1kF5UoB2nY9zeYp4UQCdcf

# 1.0 Logistic Regression
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.metrics import (
    accuracy_score, confusion_matrix, classification_report,
    precision_score, recall_score, f1_score
)
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFE
from imblearn.over_sampling import SMOTE
import seaborn as sns
import matplotlib.pyplot as plt
import missingno as msno

# 1. Upload dataset
url = "https://drive.google.com/uc?id=1GotITHFDZ7tftThE4Wdw0PkeEsCAxxAJ"
df = pd.read_excel(url)

# 2. Filter xteenmom = 1 or 2
df = df[df['xteenmom'].isin([1, 2])]

# 3. Select relevant variables
predictors = [
    'married', 'educat', 'wealthindex', 'food_sleephungy',
    'anyinfantdeaths', 'anyu5deaths',
    'mh1', 'mh2', 'mh3', 'mh4', 'mh5', 'mh6', 'mh7', 'mh8',
    'p75mhscore', 'loans'
]
target = 'anychilddead'

df = df[predictors + [target]]

print("\n====================")
print("ðŸ“Œ Logistic Regression Results")
print("====================")

# 4. Check and drop missing values
print("Missing values per column:\n", df.isnull().sum())
print("Total missing values:", df.isnull().sum().sum())
msno.matrix(df)
df = df.dropna()

# 5. Split into predictors and target
X = df[predictors]
y = df[target]

# 6. Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 7. RFE - Recursive Feature Elimination
base_model = LogisticRegression(class_weight='balanced', max_iter=1000)
rfe = RFE(estimator=base_model, n_features_to_select=10)
X_rfe = rfe.fit_transform(X_scaled, y)

# 8. SMOTE - Oversampling minority class
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_rfe, y)

# 9. Hyperparameter tuning using GridSearchCV
param_grid = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l2'],  # 'l1' if using solver='liblinear'
    'solver': ['lbfgs']
}
grid_search = GridSearchCV(LogisticRegression(class_weight='balanced', max_iter=1000),
                           param_grid, cv=5, scoring='f1')
grid_search.fit(X_resampled, y_resampled)
best_model = grid_search.best_estimator_
print("Best Parameters:", grid_search.best_params_)

# 10. Cross-validation metrics
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
acc = cross_val_score(best_model, X_resampled, y_resampled, cv=cv, scoring='accuracy').mean()
prec = cross_val_score(best_model, X_resampled, y_resampled, cv=cv, scoring='precision').mean()
rec = cross_val_score(best_model, X_resampled, y_resampled, cv=cv, scoring='recall').mean()
f1 = cross_val_score(best_model, X_resampled, y_resampled, cv=cv, scoring='f1').mean()



print("\n=== Cross-Validated Metrics (After SMOTE + GridSearchCV) ===")
print(f"Accuracy:  {acc:.3f}")
print(f"Precision: {prec:.3f}")
print(f"Recall:    {rec:.3f}")
print(f"F1 Score:  {f1:.3f}")
print("=============================================================\n")

# 11. Train/test split on resampled data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# 12. Train best model
best_model.fit(X_train, y_train)

# 13. Predict and evaluate
y_pred = best_model.predict(X_test)

print("Test Set Metrics:")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))

print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# 14. Confusion Matrix Visualization
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Death', 'Death'], yticklabels=['No Death', 'Death'])
plt.title('Confusion Matrix of Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# 15. Metric Bar Chart
metrics = {
    'Accuracy': accuracy_score(y_test, y_pred),
    'Precision': precision_score(y_test, y_pred),
    'Recall': recall_score(y_test, y_pred),
    'F1 Score': f1_score(y_test, y_pred)
}

plt.figure(figsize=(9, 5))
custom_colors = ['pink', 'skyblue', 'mediumpurple', 'mediumseagreen']
sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette=custom_colors)
for i, (metric, value) in enumerate(metrics.items()):
    plt.text(i, value + 0.02, f"{value:.2f}", ha='center', va='bottom', fontsize=12)
plt.title('Logistic Regression Model Performance Metrics')
plt.ylim(0, 1.1)
plt.ylabel('Score')
plt.show()

"""# 2.0 Decision Tree"""

# Step 1: Import Libraries
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import (
    precision_score, recall_score, f1_score, accuracy_score,
    confusion_matrix, classification_report
)
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline  # imblearn's version of Pipeline

# Step 2: Load Dataset
url = "https://drive.google.com/uc?id=1GotITHFDZ7tftThE4Wdw0PkeEsCAxxAJ"
df = pd.read_excel(url, engine='openpyxl')

# Step 3: Filter for Teen Mothers Only
df = df[df['xteenmom'].isin([1, 2])]

# Step 4: Select relevant variables
predictors = [
    'married', 'educat', 'wealthindex', 'food_sleephungy',
    'anyinfantdeaths', 'anyu5deaths',
    'mh1', 'mh2', 'mh3', 'mh4', 'mh5', 'mh6', 'mh7', 'mh8',
    'p75mhscore', 'loans'
]
target = 'anychilddead'

df = df[predictors + [target]]

print("\n====================")
print("ðŸ“Œ Decision Tree Results")
print("====================")

# Step 4.5: Check Missing Values
print("Missing values per column:")
print(df.isnull().sum())
print("-" * 40)

print("Columns with missing values:")
print(df.isnull().sum()[df.isnull().sum() > 0])
print("-" * 40)

print("Total missing values in entire DataFrame:", df.isnull().sum().sum())
print("-" * 40)

print("Rows with missing values:")
print(df[df.isnull().any(axis=1)])
print("-" * 40)

# Drop rows with missing values
df = df.dropna()

# 5. Split into predictors and target
X = df[predictors]
y = df[target]

# Step 6: Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Step 7: Create SMOTE + Decision Tree Pipeline
smote = SMOTE(random_state=42)
tree = DecisionTreeClassifier(class_weight='balanced', random_state=42)

pipeline = Pipeline([
    ('smote', smote),
    ('clf', tree)
])

# Step 8: Set up GridSearchCV
param_grid = {
    'clf__max_depth': [3, 5, 7, 9],
    'clf__min_samples_leaf': [1, 2, 4],
    'clf__criterion': ['gini', 'entropy']
}

grid = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    scoring='f1',
    cv=5,
    n_jobs=-1,
    verbose=1
)

# Step 9: Fit GridSearchCV on Training Data
grid.fit(X_train, y_train)
best_model = grid.best_estimator_

# Step 10: Predict with Threshold = 0.35 on Test Set
y_proba = best_model.predict_proba(X_test)[:, 1]
threshold = 0.35
y_pred = (y_proba >= threshold).astype(int)

# Step 11: Evaluation Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print("\nBest Parameters from GridSearchCV:", grid.best_params_)
print(f"\nEvaluation on Test Set (Threshold = {threshold}):")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("\nConfusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Step 12: Confusion Matrix Visualization
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Death', 'Death'],
            yticklabels=['No Death', 'Death'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title("Confusion Matrix of Decision Tree")
plt.tight_layout()
plt.show()

# Step 13: Visualize the Best Decision Tree
final_tree = best_model.named_steps['clf']
plt.figure(figsize=(18, 8))
plot_tree(
    final_tree,
    feature_names=X.columns,
    class_names=['No Death', 'Death'],
    filled=True, rounded=True, fontsize=9
)
plt.title("Best Decision Tree from GridSearchCV (Threshold = 0.35)")
plt.tight_layout()
plt.show()

# Step 14: Print Tree Rules
print("\nDecision Tree Rules:\n")
print(export_text(final_tree, feature_names=list(X.columns)))

# Step 15: Bar Chart for Final Metrics
metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
values = [accuracy, precision, recall, f1]

plt.figure(figsize=(7, 4))
bars = plt.bar(metrics, values, color=['#66c2a5', '#fc8d62', '#8da0cb', '#ffd92f'])
plt.ylim(0, 1)
plt.title("Decision Tree Model Performance Metrics")
plt.ylabel("Score")

for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.02, f'{yval:.2f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

from sklearn.model_selection import StratifiedKFold

# Step 16: Stratified 5-Fold Cross-Validation (for reporting)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

cv_precision = []
cv_recall = []
cv_f1 = []
cv_accuracy = []

for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):
    X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]
    y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]

    # Create new pipeline for each fold (with SMOTE and best hyperparams)
    model_cv = Pipeline([
        ('smote', SMOTE(random_state=42)),
        ('clf', DecisionTreeClassifier(
            class_weight='balanced',
            criterion=grid.best_params_['clf__criterion'],
            max_depth=grid.best_params_['clf__max_depth'],
            min_samples_leaf=grid.best_params_['clf__min_samples_leaf'],
            random_state=42
        ))
    ])

    model_cv.fit(X_train_cv, y_train_cv)
    y_proba_cv = model_cv.predict_proba(X_val_cv)[:, 1]
    y_pred_cv = (y_proba_cv >= 0.35).astype(int)

    cv_precision.append(precision_score(y_val_cv, y_pred_cv))
    cv_recall.append(recall_score(y_val_cv, y_pred_cv))
    cv_f1.append(f1_score(y_val_cv, y_pred_cv))
    cv_accuracy.append(accuracy_score(y_val_cv, y_pred_cv))

# Print average metrics across folds
print("\nStratified 5-Fold Cross-Validation (Threshold = 0.35):")
for i in range(5):
    print(f"Fold {i+1} - Accuracy: {cv_accuracy[i]:.4f}, Precision: {cv_precision[i]:.4f}, Recall: {cv_recall[i]:.4f}, F1: {cv_f1[i]:.4f}")

print("\nAverage Across Folds:")
print(f"Avg Accuracy:  {np.mean(cv_accuracy):.4f}")
print(f"Avg Precision: {np.mean(cv_precision):.4f}")
print(f"Avg Recall:    {np.mean(cv_recall):.4f}")
print(f"Avg F1 Score:  {np.mean(cv_f1):.4f}")

"""# 3.0 Random Forest"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, ConfusionMatrixDisplay
)
from sklearn.feature_selection import RFE
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline  # For SMOTE compatibility

# Load data
url = "https://drive.google.com/uc?id=1GotITHFDZ7tftThE4Wdw0PkeEsCAxxAJ"
df = pd.read_excel(url)

# Filter for teen mothers
df = df[df['xteenmom'].isin([1, 2])]

# Select predictors and target
target = 'anychilddead'
predictors = [
    'married', 'educat', 'wealthindex', 'food_sleephungy', 'anyinfantdeaths', 'anyu5deaths',
    'mh1', 'mh2', 'mh3', 'mh4', 'mh5', 'mh6', 'mh7', 'mh8', 'p75mhscore', 'loans'
]
df_model = df[[target] + predictors].dropna()
X = df_model[predictors]
y = df_model[target]

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Pipeline: StandardScaler + RFE + SMOTE + RandomForest + GridSearchCV
rf = RandomForestClassifier(random_state=42, class_weight='balanced')

# Apply RFE
rfe_selector = RFE(estimator=rf, n_features_to_select=10)

# Create pipeline with SMOTE
pipeline = ImbPipeline(steps=[
    ('scaler', StandardScaler()),
    ('rfe', rfe_selector),
    ('smote', SMOTE(random_state=42)),
    ('classifier', rf)
])

# Grid search for hyperparameter tuning
param_grid = {
    'classifier__n_estimators': [100, 200],
    'classifier__max_depth': [None, 10, 20],
    'classifier__min_samples_split': [2, 5],
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

grid = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    cv=cv,
    scoring='f1',
    verbose=1,
    n_jobs=-1
)

# Fit model
grid.fit(X_train, y_train)

# Best estimator
best_model = grid.best_estimator_
y_pred = best_model.predict(X_test)

# Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

# Print results
print("\n====================")
print("ðŸ“Œ Random Forest Results")
print("====================")

print("âœ… Best Parameters from GridSearchCV:", grid.best_params_)
print("\nðŸŽ¯ Evaluation Metrics on Test Set:")
print(f"Accuracy : {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall   : {recall:.4f}")
print(f"F1 Score : {f1:.4f}")

print("\nðŸ“‹ Classification Report:")
print(classification_report(y_test, y_pred, digits=4))

# Bar Chart for Evaluation Metrics
plt.figure(figsize=(6, 4))
metrics = [accuracy, precision, recall, f1]
metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
sns.barplot(x=metric_names, y=metrics, palette='Set2')
plt.ylim(0, 1)
plt.title("Random Forest Model Performance Metrics")
for i, v in enumerate(metrics):
    plt.text(i, v + 0.02, f"{v:.2f}", ha='center', fontweight='bold')
plt.ylabel("Score")
plt.tight_layout()
plt.show()

# Confusion Matrix Plot
plt.figure(figsize=(5, 4))
ConfusionMatrixDisplay(confusion_matrix=cm).plot(cmap='Blues', values_format='d')
plt.title("Confusion Matrix of Random Forest")
plt.grid(False)
plt.show()

"""# 4.0 Support Vector Machine (SVM)"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_validate
from sklearn.svm import SVC
from sklearn.metrics import (
    precision_score, recall_score, f1_score, accuracy_score,
    confusion_matrix, ConfusionMatrixDisplay, classification_report, make_scorer
)
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline  # imblearn's pipeline, not sklearn

# Step 1: Load data from Google Drive
url = "https://drive.google.com/uc?id=1GotITHFDZ7tftThE4Wdw0PkeEsCAxxAJ"
df = pd.read_excel(url)

# Step 2: Filter for teen mothers only
df = df[df['xteenmom'].isin([1, 2])]

# Step 3: Define target and predictors
target = 'anychilddead'
predictors = [
    'married', 'educat', 'wealthindex', 'food_sleephungy', 'anyinfantdeaths', 'anyu5deaths',
    'mh1', 'mh2', 'mh3', 'mh4', 'mh5', 'mh6', 'mh7', 'mh8', 'p75mhscore', 'loans'
]

# Step 4: Drop missing
df_model = df[[target] + predictors].dropna()
X = df_model[predictors]
y = df_model[target]

# Step 5: Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Step 6: Build pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('svm', SVC(class_weight='balanced'))
])

# Step 7: Define parameter grid for GridSearchCV
param_grid = {
    'svm__kernel': ['linear', 'rbf'],
    'svm__C': [0.1, 1, 10],
    'svm__gamma': ['scale', 'auto']
}

# Step 8: Scoring & CV
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1': make_scorer(f1_score)
}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Step 9: Grid Search
grid_search = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    scoring='f1',
    cv=cv,
    n_jobs=-1,
    verbose=1
)
grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_

# Step 10: Evaluate on Test Set
y_pred = best_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print("\n====================")
print("ðŸ“Œ Support Vector Machine Results")
print("====================")

print("Best Parameters from GridSearchCV:")
print(grid_search.best_params_)

print("\nEvaluation Metrics on Test Set (SVM + SMOTE + GridSearchCV):")
print(f"Accuracy:  {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")
print("Confusion Matrix:")
print(cm)

print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=4))

# Step 11: Plot Confusion Matrix
plt.figure(figsize=(5, 4))
ConfusionMatrixDisplay(confusion_matrix=cm).plot(cmap='Blues', values_format='d')
plt.title("Confusion Matrix of Support Vector Machine")
plt.grid(False)
plt.show()

# Step 12: Cross-validation scores (with same best model)
cv_results = cross_validate(
    estimator=best_model,
    X=X_train,
    y=y_train,
    cv=cv,
    scoring=scoring,
    return_train_score=False
)

# Step 13: Print CV metrics
print("\nCross-Validation Metrics (Train Set):")
for metric in ['accuracy', 'precision', 'recall', 'f1']:
    mean_score = cv_results[f'test_{metric}'].mean()
    std_score = cv_results[f'test_{metric}'].std()
    print(f"{metric.capitalize()}: {mean_score:.4f} Â± {std_score:.4f}")


# Step 14: Bar Chart of CV Metrics (Updated)
plt.figure(figsize=(6, 4))
metrics = ['accuracy', 'precision', 'recall', 'f1']
scores = [cv_results[f'test_{m}'].mean() for m in metrics]

ax = sns.barplot(x=metrics, y=scores, palette="viridis")

# Remove gridlines
ax.grid(False)

# Add values on top of each bar
for i, v in enumerate(scores):
    ax.text(i, v + 0.01, f"{v:.2f}", ha='center', va='bottom', fontsize=10)

plt.ylim(0, 1)
plt.title("Support Vector Machine Model Performance Metrics")
plt.ylabel("Score")
plt.xlabel("Metric")
plt.show()

"""# 5.0 Naive Bayes"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, precision_recall_curve
)
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline  # To handle SMOTE + scaling

# === Load and clean data ===
url = "https://drive.google.com/uc?id=1GotITHFDZ7tftThE4Wdw0PkeEsCAxxAJ"
df = pd.read_excel(url)
df = df[df['xteenmom'].isin([1, 2])]
df = df.dropna()

# === Features and target ===
features = ['married', 'educat', 'wealthindex', 'food_sleephungy',
            'anyinfantdeaths', 'anyu5deaths', 'p75mhscore', 'loans'] + [f'mh{i}' for i in range(1, 9)]
X = df[features]
y = df['anychilddead']

# === Train/test split ===
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

# === Pipeline: StandardScaler -> SMOTE -> GaussianNB ===
pipeline = ImbPipeline(steps=[
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('classifier', GaussianNB())
])

# === GridSearchCV for var_smoothing ===
param_grid = {
    'classifier__var_smoothing': np.logspace(-11, -7, 5)
}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

grid = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    cv=cv,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

# === Fit the pipeline ===
grid.fit(X_train, y_train)

# === Predict probabilities and optimize threshold ===
y_proba = grid.predict_proba(X_test)[:, 1]
precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
best_idx = f1_scores.argmax()
best_threshold = thresholds[best_idx]
y_pred = (y_proba >= best_threshold).astype(int)

# === Evaluation ===
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, zero_division=0)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

# === Print results ===
print("\n====================")
print("ðŸ“Œ Naive Bayes Results")
print("====================")

print("âœ… Best Parameters from GridSearchCV:", grid.best_params_)
print("\n=== Metrics ===")
print(f"Accuracy : {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall   : {recall:.4f}")
print(f"F1 Score : {f1:.4f}")
print("\n=== Confusion Matrix ===")
print(cm)

# === Bar Chart (4 Metrics) ===
metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
scores = [accuracy, precision, recall, f1]
colors = ['purple', 'blue', 'green', 'red']

plt.figure(figsize=(7, 5))
bars = plt.bar(metrics, scores, color=colors)
for bar in bars:
    height = bar.get_height()
    plt.annotate(f"{height:.2f}", xy=(bar.get_x() + bar.get_width() / 2, height),
                 xytext=(0, 3), textcoords="offset points",
                 ha='center', va='bottom', fontsize=10)

plt.ylim(0, 1)
plt.ylabel("Score")
plt.title("Naive Bayes Model Performance Metrics")
plt.tight_layout()
plt.show()

# === Confusion Matrix Heatmap ===
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Child Death', 'Child Death'],
            yticklabels=['No Child Death', 'Child Death'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix of Naive Bayes')
plt.tight_layout()
plt.show()

"""# 6.0 XGBoost"""

#!pip install xgboost imbalanced-learn openpyxl

import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.metrics import (
    precision_score, recall_score, f1_score, accuracy_score,
    confusion_matrix, ConfusionMatrixDisplay, classification_report, make_scorer
)
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

# Step 1: Load data
url = "https://drive.google.com/uc?id=1GotITHFDZ7tftThE4Wdw0PkeEsCAxxAJ"
df = pd.read_excel(url)

# Step 2: Filter for teen mothers
df = df[df['xteenmom'].isin([1, 2])]

# Step 3: Define target and predictors
target = 'anychilddead'
predictors = [
    'married', 'educat', 'wealthindex', 'food_sleephungy', 'anyinfantdeaths', 'anyu5deaths',
    'mh1', 'mh2', 'mh3', 'mh4', 'mh5', 'mh6', 'mh7', 'mh8', 'p75mhscore', 'loans'
]

# Step 4: Drop missing and split
df_model = df[[target] + predictors].dropna()
X = df_model[predictors]
y = df_model[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Step 5: Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 6: Apply SMOTE on training data
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

# Step 7: Define model with class weight
class_counts = Counter(y_train)
scale = class_counts[0] / class_counts[1]  # majority / minority

xgb = XGBClassifier(eval_metric='logloss', use_label_encoder=False, scale_pos_weight=scale, random_state=42)

# Step 8: GridSearchCV setup
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8, 1]
}
scoring = {
    'accuracy': 'accuracy',
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1': make_scorer(f1_score)
}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
grid = GridSearchCV(xgb, param_grid, scoring='f1', cv=cv, n_jobs=-1)
grid.fit(X_train_smote, y_train_smote)

best_model = grid.best_estimator_

# Step 9: Evaluate on test set
y_pred = best_model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print("\n====================")
print("ðŸ“Œ XGBoost Results")
print("====================")

print("Evaluation Metrics (Test Set):")
print(f"Accuracy:  {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")
print("\nConfusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_pred, digits=4))

# Step 10: Cross-validation metrics using best model
cv_scores = {}
for metric, scorer in scoring.items():
    scores = cross_val_score(best_model, X_train_scaled, y_train, cv=cv, scoring=scorer)
    cv_scores[metric] = scores.mean()

print("\nCross-Validation Metrics (on Original Training Set):")
for metric, score in cv_scores.items():
    print(f"{metric.capitalize()}: {score:.4f}")

# Step 11: Confusion Matrix Plot
plt.figure(figsize=(5, 4))
ConfusionMatrixDisplay(confusion_matrix=cm).plot(cmap='Blues', values_format='d')
plt.title("Confusion Matrix of XGBoost")
plt.grid(False)
plt.show()

# Step 12: Bar chart of CV scores
plt.figure(figsize=(6, 4))
sns.barplot(x=list(cv_scores.keys()), y=list(cv_scores.values()), palette='Set2')
plt.ylim(0, 1)
for i, v in enumerate(cv_scores.values()):
    plt.text(i, v + 0.02, f"{v:.2f}", ha='center', fontweight='bold')
plt.ylabel("Score")
plt.title("XGBoost Model Performance Metrics")
plt.grid(False)
plt.show()